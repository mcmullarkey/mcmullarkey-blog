---
title: "Assessing the Scalability of Birdwatch"
description: "An Initial Investigation"
author: "Michael Mullarkey"
date: "2022-11-13"
format: 
  html:
    code-fold: true
    toc: true
categories: [Trust and Safety, Python, Twitter]
jupyter: python3
---

# Why Should I Care?

[A recent Washington Post article](https://www.washingtonpost.com/technology/2022/11/09/twitter-birdwatch-factcheck-musk-misinfo/){target="_blank"} dove deep on Twitter's crowd-sourced fact-checking program Birdwatch. Twitter's owner [touted the volunteer-driven initiative](https://twitter.com/elonmusk/status/1588933974470332418?s=20&t=YX1oXKVQwUw54pJVyyNHEg){target="_blank"} around the same time [15% of Trust and Safety staff were laid off.](https://www.theverge.com/2022/11/4/23441404/twitter-trust-safety-staff-layoffs-content-moderation){target="_blank"} A week later a large number of content moderation contractors^[Including a contractor making critical changes to child safety workflows https://twitter.com/CaseyNewton/status/1591608307927556096?s=20&t=4lurUg2rjlnq6mZ8xqquNQ] were [fired without notice](https://mobile.twitter.com/CaseyNewton/status/1591608302076858371){target="_blank"} and the head of Trust and Safety [resigned](https://www.theverge.com/2022/11/10/23452133/twitter-executives-trust-safety-sales-ads-yoel-roth-robin-wheeler){target="_blank"}.<br>
<br>
Data related to trust and safety are rarely open for audit, but Birdwatch^[Now referred to by some people as Community Notes, though I'll be using Birdwatch throughout] has provided open source data, code, and documentation since it started as a [small pilot program in January 2021](https://blog.twitter.com/en_us/topics/product/2021/introducing-birdwatch-a-community-based-approach-to-misinformation){target="_blank"}. I decided to do an initial assessment of the open data with an eye toward Birdwatch's scalability as a content moderation tool.

# Load Packages

We don't need a lot of Python packages to do this analysis,^[And if you don't care about the code you can ignore it. You don't need to know Python to read this post!] and I try to keep my dependencies as light as possible without making my life a nightmare.

```{python}

import numpy as np
import pandas as pd

```

# Import Birdwatch Data

We can download the data from [this page](https://twitter.com/i/birdwatch/download-data){target="_blank"}, where the Birdwatch data is updated daily. <br>
<br>
I downloaded this data on November 13, 2022. If you're accessing this data in the future the analyses will not exactly reproduce since new data will be included. You can feel free to grab the data I'm using from [the Github repo for this post](https://github.com/mcmullarkey/mcmullarkey-blog/tree/main/posts/2022_11_13_birdwatch_data){target="_blank"}.<br>
<br>
First we'll make a quick function to read the .tsv files in as Pandas Data Frames.

```{python}

def read_tsv(path):
  data = pd.read_csv(path, sep='\t')
  return data

```

Then we'll apply that function to all of the Birdwatch data.

```{python}

paths = ["notes-00000.tsv", "noteStatusHistory-00000.tsv", "ratings-00000.tsv"]

initial_dfs = []
for path in paths:
  initial_data = read_tsv(path)
  initial_dfs.append(initial_data)

```

# Exploring Birdwatch Data

To make it easier to explore each data frame, we'll assign them to separate objects outside the list. The documentation for all three datasets provided by Birdwatch is [here.](https://twitter.github.io/communitynotes/download-data/){target="_blank"}

```{python}

initial_notes, initial_history, initial_ratings = initial_dfs

```

Before I move forward, one huge positive of this project is that data is available for outside research. Allowing external audits of content moderation approaches is tricky, and I commend the Birdwatch folks for their transparency so far. 

## A Quick Primer on How Birdwatch Works

Any Twitter user can join Birdwatch with the ultimate goal of adding notes to tweets. Those notes can fact-check, provide additional context, and in theory deter disinformation. <br>
<br>
There are checks and balances on the Birdwatch system geared toward stopping bad-faith actors. While anyone can join Birdwatch you cannot write your own initial notes on tweets when you join. First, you must consistently submit ratings of others' initial notes that agree with the other Birdwatch members' general consensus. <br>
<br>
Those ratings from Birdwatch members also serve as a powerful bottleneck for which initial notes ultimately appear on tweets. The ranking for whether a note is "helpful" enough to apply to a tweet is more complex than a majority vote among Birdwatch members. <br>
<br>
Instead, initial notes that receive a few positive ratings from people who normally disagree on their ratings are more likely to be rated as helpful than initial notes that receive many positive ratings from people who normally agree.<br>
<br>
This system is known as [bridge-based ranking](https://www.belfercenter.org/publication/bridging-based-ranking){target="_blank"} and algorithmically prioritizes this form of consensus over potential alternatives The Washington Post article notes this approach is unlikely to scale, especially in ["an era when left and right often lack a shared set of facts."](https://www.washingtonpost.com/technology/2022/11/09/twitter-birdwatch-factcheck-musk-misinfo/){target="_blank"} <br>
<br>
To see how well this approach does or does not scale right now, let's dive into the data.

## How Many Tweets Have Initial Notes?

```{python}

# Grouping by tweetId and counting number of notes per tweet

tweets_initial_notes = initial_notes.groupby("tweetId").count()

# Use f-strings to get key info

print(f"Birdwatchers have put initial notes on {len(tweets_initial_notes)} tweets since January of 2021.")

```

## How Does This Compare to the Total Volume of Tweets?

For context, there are approximately [500,000,000 tweets sent per day](https://thesocialshepherd.com/blog/twitter-statistics){target="_blank"}. <br>
<br>
Even if we assume that only the top 0.1% of tweets require the scrutiny of Birdwatch that would mean 500 tweets should be considered for notes per day.<br>
<br>
We can be extra generous and say fewer tweets than that might require notes, but we'd still expect around 500 notes per day. How many days in the Birdwatch data meet that criteria?

```{python}

# Converting to date instead of milliseconds since epoch

initial_notes["dateCreated"] = pd.to_datetime(initial_notes["createdAtMillis"], unit = "ms").dt.date

# Counting the number of notes per day

tweet_initial_dates = initial_notes.groupby(["dateCreated"]).count()

# Finding the earliest date

min_tweet_date = tweet_initial_dates.index.min()

# Finding how many dates where over 500 notes or more were created

days_500_per = tweet_initial_dates[tweet_initial_dates.tweetId >= 500]

# Getting value of only date where >500 notes were created

only_date_over = days_500_per.index.values

print(f"In all Birdwatch data going back to {min_tweet_date}, there was {len(days_500_per)} day where at least 500 notes were written - {only_date_over[0]}")

```

Even with relaxed criteria, there was only 1 day at the very beginning of Birdwatch where the community reviewed approximately 0.1% of all tweets in a day. <br>
<br>
This relatively low review volume is understandable given Birdwatch is an almost all-volunteer effort. However, this precedent of not operating at scale becomes concerning if Birdwatch is expected to play a large role in preventing disinformation on the platform.

## How Many Initial Notes Need More Ratings to Determine Their Helpfulness?

```{python}

# Getting status for which notes were rated as helpful or not

note_status = initial_history[["noteId","currentStatus"]]

# Seeing what percentage of notes with evaluations need more evaluation

status_counts = note_status.currentStatus.value_counts()

pd.DataFrame(status_counts)\
.assign(percent = lambda x: (x["currentStatus"] / x["currentStatus"].sum()) * 100)\
.round(2)

```

Another indication that an all-volunteer effort isn't enough to scale this form of content moderation - nearly 87% of initial notes need more ratings to determine whether they could be helpful or not. <br>
<br>
All initial notes start out as "Needs More Ratings" until they've received at least 5 ratings, and it appears a vast majority of notes never meet that threshold. <br>
<br>
There could be multiple reaons for this, ranging from charitable^[The Birdwatch community actively doesn't bother rating initial notes from obvious trolls, notes on low value tweets, or some other combination of undesirable features] to less so.^[There just aren't enough people who can volunteer their time to such an intensive effort so most initial notes never receive enough ratings] There could be reasons internal to the Birdwatch community I'm unaware of that drive this pattern. <br>
<br>
And no matter what, the current Birdwatch system is failing to identify whether a vast majority initial notes are helpful. This is true even though the volume of initial notes is infentisimal compared to the total volume of tweets. If more initial notes were written to better keep up with overall tweet volume, there's a chance this lack of ratings problem would be exacerbated.

## Example: The Tweet With the Most Initial Notes Had No Notes with Enough Ratings

```{python}

print(f"The tweet with the most initial notes had {tweets_initial_notes.noteId.max()} notes.")

# tweets_initial_notes[tweets_initial_notes.noteId == 58]
# Can use this website to get tweets from tweetId without using the API https://www.bram.us/2017/11/22/accessing-a-tweet-using-only-its-id-and-without-the-twitter-api/

```

The tweet with the most initial notes was by [Rep. Alexandria Ocasio-Cortez](https://twitter.com/AOC/status/1354848253729234944){target="_blank"} in response to Senator Ted Cruz. The tweet touched on the trading platform Robinhood's decision to prevent retail investors from trading certain stocks and the January 6th insurrection.<br>
<br>
This tweet ultimately did not have a note attached to it. <br>
<br>
Tweets could not have a note attached to them for 2 reasons: <br>
1. [There is no note rated as helpful](https://twitter.github.io/communitynotes/notes-on-twitter/){target="_blank"} <br>
2. There is at least one note rated as helpful but the Tweet [is not marked as "potentially misleading"](https://twitter.github.io/communitynotes/ranking-notes/){target="_blank"}<br>
<br>
In this case no initial note was rated as helpful, and to boot none of the initial notes had enough ratings to even be considered.

```{python}

# Getting all noteIds in reference to the AOC tweet into a list

aoc_note_ids = initial_notes[initial_notes["tweetId"] == 1354848253729234944].noteId.to_list()

# Filtering the note history based on this list and counting values

initial_history[initial_history["noteId"].isin(aoc_note_ids)].currentStatus.value_counts()

```

Even if you believe this tweet should not have received a note,^[Cards on the table, I don't think this tweet needs a note] it's troubling that its status remained up in the air rather than seeing a definitive "not helpful" label applied to all initial notes.

## Would More Birdwatch Members Solve All These Problems?

The two previous scalability issues could, at least in principle, be solved by having a lot more people joining Birdwatch. More initial notes could be written, more initial notes could receive ratings, and the system could achieve at least some scalability.<br>
<br>
However, there are reasons to believe that using its current standards more members could actually make Birdwatch *less* scalable. <br>
<br>
Think back to the example of the tweet with the most initial notes ever. Lots of people wrote initial notes, but nowhere near enough people rated all those initial notes. <br>
<br>
It's possible Birdwatch has better procedures in place now, but it seems like more Birdwatch members could exacerbate this coordination problem. Too many people writing initial notes, and - after the initial probationary period - not enough people rating initial notes.

# Summary of Findings
1. Birdwatch currently reviews an extraordinarily low volume of overall tweets <br>
2. A vast majority of initial notes do not receive enough ratings to assess their helpfulness <br>
3. Birdwatch is not likely to scale well in its current form

# Suggested Action Items
1. Pay people to perform Birdwatch's functions<br>
<br>
Paying people fair wages and professionalizing the approach to which tweets receive notes would likely yield a fantastic return on investment. This approach would help increase the volume of initial notes, ensure enough ratings for each initial note, and provide crucial coordination capabilities such as consistently prioritizing high-value tweets.^[I think the Birdwatch community has on balance elected to prioritize high-value tweets such as misleading tweets from Twitter's owner. I'm also certain Twitter has more data that could help a system like this nip misinformation in the bud *before* it's accrued millions of impressions] If Twitter wants to retain a bottom-up, participatory research style branch of content moderation they can do so. However, I recommend they consult with Trust and Safety professionals along with Birdwatch members on what form that participatory research should take. <br>
<br>
2. Re-situate Birdwatch as one of many content moderation tools<br>
<br>
Birdwatch doesn't need to massively scale to be useful. It could function well as a small part of a suite of content moderation tools. However, if Twitter relies on the current iteration of Birdwatch as its central content moderation tool I would expect massive content moderation failures. Not because Birdwatch community members are performing poorly, but because they would be community members performing the tasks of many teams' worth of employees.

# Conclusion

This analysis is only possible because Birdwatch has open data. Trust and Safety measures require some procedures be kept under lock and key, and the considered transparency baked into Birdwatch's approach since January 2021 is admirable.  The [now-defunct](https://gizmodo.com/twitter-layoffs-elon-musk-ai-ethics-1849743051){target="_blank"} META team at Twitter also made [considered transparency a consistent practice](https://blog.twitter.com/en_us/topics/company/2021/introducing-responsible-machine-learning-initiative){target="_blank"}. <br>
<br>
The more teams follow these examples the better we'll be able to moderate content in helpful, just ways.<br>
<br>
There are many stones still left to turn in this data. For example, I think^[But haven't directly confirmed!] that a vast majority^[And maybe all] of the tweets with initial notes are in English. Someone could look into that and contextualize the volume of tweets Birdwatch hasn't even attempted to moderate. <br>
<br>
I hope I can inspire at least a couple of other people to take a closer look, and if you find anything interesting please [get in touch](https://mcmullarkey.github.io/){target="_blank"}.
