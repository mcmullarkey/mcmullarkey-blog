{
  "hash": "e4d8d406993c66f9751748f80e964fbe",
  "result": {
    "markdown": "---\ntitle: Assessing the Scalability of Birdwatch\ndescription: An Initial Investigation\nauthor: Michael Mullarkey\ndate: '2022-11-13'\nformat:\n  html:\n    code-fold: true\n    toc: true\ncategories:\n  - Trust and Safety\n  - Python\n  - Twitter\n---\n\n# Why Should I Care?\n\n[A recent Washington Post article](https://www.washingtonpost.com/technology/2022/11/09/twitter-birdwatch-factcheck-musk-misinfo/){target=\"_blank\"} dove deep on Twitter's crowd-sourced fact-checking program Birdwatch. Twitter's owner [touted the volunteer-driven initiative](https://twitter.com/elonmusk/status/1588933974470332418?s=20&t=YX1oXKVQwUw54pJVyyNHEg){target=\"_blank\"} around the same time [15% of Trust and Safety staff were laid off.](https://www.theverge.com/2022/11/4/23441404/twitter-trust-safety-staff-layoffs-content-moderation){target=\"_blank\"} A week later a large number of content moderation contractors^[Including a contractor making critical changes to child safety workflows https://twitter.com/CaseyNewton/status/1591608307927556096?s=20&t=4lurUg2rjlnq6mZ8xqquNQ] were [fired without notice](https://mobile.twitter.com/CaseyNewton/status/1591608302076858371){target=\"_blank\"} and the head of Trust and Safety [had resigned](https://www.theverge.com/2022/11/10/23452133/twitter-executives-trust-safety-sales-ads-yoel-roth-robin-wheeler){target=\"_blank\"}.<br>\n<br>\nData related to trust and safety decisions like these are rarely open for audit, but Birdwatch^[Now referred to by some people as Community Notes, though I'll be using Birdwatch throughout] has provided open source data, code, and documentation since it started as a [small pilot program in January 2021](https://blog.twitter.com/en_us/topics/product/2021/introducing-birdwatch-a-community-based-approach-to-misinformation){target=\"_blank\"}. I decided to do an initial assessment of the open data with an eye toward Birdwatch's scalability as a content moderation tool.\n\n# Load Packages\n\nWe don't need a lot of Python packages to do this analysis, and I try to keep my dependencies as light as possible without making my life a nightmare.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n# Import Birdwatch Data\n\nWe can download the data from [this page](https://twitter.com/i/birdwatch/download-data){target=\"_blank\"}, where the Birdwatch data is updated daily. <br>\n<br>\nI downloaded this data on November 13, 2022. If you're accessing this data in the future the analyses will not exactly reproduce since new data will be included. You can feel free to grab the data I'm using from the Github repo for this post.<br>\n<br>\nFirst we'll make a quick function to read the .tsv files in as Pandas Data Frames.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef read_tsv(path):\n  data = pd.read_csv(path, sep='\\t')\n  return data\n```\n:::\n\n\nThen we'll apply that function to all of the Birdwatch data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\npaths = [\"notes-00000.tsv\", \"noteStatusHistory-00000.tsv\", \"ratings-00000.tsv\"]\n\ninitial_dfs = []\nfor path in paths:\n  initial_data = read_tsv(path)\n  initial_dfs.append(initial_data)\n```\n:::\n\n\n# Exploring Birdwatch Data\n\nTo make it easier to explore each data frame, we'll assign them to separate objects outside the list. The documentation for all three datasets provided by Birdwatch is [here.](https://twitter.github.io/communitynotes/download-data/){target=\"_blank\"}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ninitial_notes, initial_history, initial_ratings = initial_dfs\n```\n:::\n\n\nBefore I move forward, one huge positive of this project is that data is available for outside research. Allowing external audits of content moderation approaches is tricky, and I commend the Birdwatch folks for their transparency so far. \n\n## A Quick Primer on How Birdwatch Works\n\nAny Twitter user can join Birdwatch with the ultimate goal of adding notes to tweets. Those notes can fact-check, provide additional context, and in theory deter disinformation. <br>\n<br>\nThere are checks and balances on the Birdwatch system geared toward stopping bad-faith actors. While anyone can join Birdwatch you cannot write your own initial notes on tweets when you join. First, you must consistently submit ratings of others' initial notes that agree with the other Birdwatcher members' general consensus. <br>\n<br>\nThose ratings from Birdwatch members also serve as a powerful bottleneck for which initial notes ultimately appear on tweets. The ranking for whether a note is \"helpful\" enough to apply to a tweet is more complex than a majority vote among Birdwatch members. <br>\n<br>\nInstead, initial notes that receive a few positive ratings from people who normally disagree on their ratings are more likely to be rated as helpful than initial notes that receive positive ratings from many people who normally agree.<br>\n<br>\nThis system is known as [bridge-based ranking](https://www.belfercenter.org/publication/bridging-based-ranking){target=\"_blank\"} and algorithmically prioritizes this form of consensus over other potential approaches. The Washington Post article notes this approach is unlikely to scale, especially in [\"an era when left and right often lack a shared set of facts.\"](https://www.washingtonpost.com/technology/2022/11/09/twitter-birdwatch-factcheck-musk-misinfo/){target=\"_blank\"} <br>\n<br>\nTo see how well this approach does or does not scale right now, let's dive into the data.\n\n## How Many Tweets Have Initial Notes?\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Grouping by tweetId and counting number of notes per tweet\n\ntweets_initial_notes = initial_notes.groupby(\"tweetId\").count()\n\n# Use f-strings to get key info\n\nprint(f\"Birdwatchers have put initial notes on {len(tweets_initial_notes)} tweets since January of 2021.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBirdwatchers have put initial notes on 28723 tweets since January of 2021.\n```\n:::\n:::\n\n\n## How Does This Compare to the Total Volume of Tweets?\n\nFor context, there are approximately [500,000,000 tweets sent per day](https://thesocialshepherd.com/blog/twitter-statistics){target=\"_blank\"}. <br>\n<br>\nEven if we assume that only the top 0.1% of tweets require the scrutiny of Birdwatch that would mean 500 tweets should be considered for notes per day.<br>\n<br>\nWe can be extra generous and say fewer tweets than that might require notes, but we'd still expect around 500 notes per day. How many days in Birdwatch data meet that criteria?\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Converting to date instead of milliseconds since epoch\n\ninitial_notes[\"dateCreated\"] = pd.to_datetime(initial_notes[\"createdAtMillis\"], unit = \"ms\").dt.date\n\n# Counting the number of notes per day\n\ntweet_initial_dates = initial_notes.groupby([\"dateCreated\"]).count()\n\n# Finding the earliest date\n\nmin_tweet_date = tweet_initial_dates.index.min()\n\n# Finding how many dates where over 500 notes or more were created\n\ndays_500_per = tweet_initial_dates[tweet_initial_dates.tweetId >= 500]\n\n# Getting value of only date where >500 notes were created\n\nonly_date_over = days_500_per.index.values\n\nprint(f\"In all Birdwatch data going back to {min_tweet_date}, there was {len(days_500_per)} day where at least 500 notes were written - {only_date_over[0]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn all Birdwatch data going back to 2021-01-23, there was 1 day where at least 500 notes were written - 2021-01-28\n```\n:::\n:::\n\n\nEven with relaxed criteria, there was only 1 day at the very beginning of Birdwatch where the community reviewed approximately 0.1% of all tweets in a day. <br>\n<br>\nThis relatively low review volume is understandable given Birdwatch is an almost all-volunteer effort. However, this precedent of not operating at scale becomes concerning if Birdwatch is expected to play a large role in preventing disinformation on the platform.\n\n## How Many Initial Notes Need More Ratings to Determine Their Helpfulness?\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Getting status for which notes were rated as helpful or not\n\nnote_status = initial_history[[\"noteId\",\"currentStatus\"]]\n\n# Seeing what percentage of notes with evaluations need more evaluation\n\nstatus_counts = note_status.currentStatus.value_counts()\n\npd.DataFrame(status_counts)\\\n.assign(percent = lambda x: (x[\"currentStatus\"] / x[\"currentStatus\"].sum()) * 100)\\\n.round(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=286}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>currentStatus</th>\n      <th>percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>NEEDS_MORE_RATINGS</th>\n      <td>14517</td>\n      <td>86.90</td>\n    </tr>\n    <tr>\n      <th>CURRENTLY_RATED_HELPFUL</th>\n      <td>1506</td>\n      <td>9.01</td>\n    </tr>\n    <tr>\n      <th>CURRENTLY_RATED_NOT_HELPFUL</th>\n      <td>683</td>\n      <td>4.09</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnother indication that an all-volunteer effort isn't enough to scale this form of content moderation - nearly 87% of initial notes need more ratings to determine whether they could be helpful or not. <br>\n<br>\nAll initial notes start out as \"Needs More Ratings\" until they've received at least 5 ratings, and it appears a vast majority of notes never meet that threshold. <br>\n<br>\nThere could be multiple reaons for this, ranging from charitable^[The Birdwatch community actively doesn't bother rating initial notes from obvious trolls, notes on low value tweets, or some other combination of undesirable features] to less so.^[There just aren't enough people who can volunteer their time to such an intensive effort so most initial notes never receive enough ratings] There could be reasons internal to the Birdwatch community I'm unaware of that drive this pattern. <br>\n<br>\nAnd no matter what, the current Birdwatch system is failing to identify whether a vast majority initial notes are helpful. This is true even though the volume of initial notes is infentisimal compared to the total volume of tweets. If more initial notes were written to better keep up with overall tweet volume, there's a chance this lack of ratings problem would be exacerbated.\n\n## Example: The Tweet With the Most Initial Notes Had No Notes with Enough Ratings\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint(f\"The tweet with the most initial notes had {tweets_initial_notes.noteId.max()} notes.\")\n\n# tweets_initial_notes[tweets_initial_notes.noteId == 58]\n# Can use this website to get tweets from tweetId without using the API https://www.bram.us/2017/11/22/accessing-a-tweet-using-only-its-id-and-without-the-twitter-api/\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe tweet with the most initial notes had 58 notes.\n```\n:::\n:::\n\n\nThe tweet with the most initial notes was by [Rep. Alexandria Ocasio-Cortez](https://twitter.com/AOC/status/1354848253729234944){target=\"_blank\"} in response to Senator Ted Cruz. The tweet touched on the trading platform Robinhood's decision to prevent retail investors from trading certain stocks and the January 6th insurrection.<br>\n<br>\nThis tweet ultimately did not have a note attached to it. <br>\n<br>\nTweets could not have a note attached to them for 2 reasons: <br>\n1. [There is no note rated as helpful](https://twitter.github.io/communitynotes/notes-on-twitter/){target=\"_blank\"} <br>\n2. There is at least one note rated as helpful but the Tweet [is not marked as \"potentially misleading\"](https://twitter.github.io/communitynotes/ranking-notes/){target=\"_blank\"}<br>\n<br>\nIn this case no initial note was rated as helpful, and to boot none of the initial notes had enough ratings to even be considered.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Getting all noteIds in reference to the AOC tweet into a list\n\naoc_note_ids = initial_notes[initial_notes[\"tweetId\"] == 1354848253729234944].noteId.to_list()\n\n# Filtering the note history based on this list and counting values\n\ninitial_history[initial_history[\"noteId\"].isin(aoc_note_ids)].currentStatus.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=288}\n```\nNEEDS_MORE_RATINGS    54\nName: currentStatus, dtype: int64\n```\n:::\n:::\n\n\nEven if you believe this tweet should not have received a note,^[Cards on the table, I don't think this tweet needs a note] it's troubling that its status remained up in the air rather than seeing a definitive \"not helpful\" label applied to all initial notes.\n\n## Would More Birdwatch Members Solve All These Problems?\n\nThe two previous scalability issues could, at least in principle, be solved by having a lot more people joining Birdwatch. More initial notes could be written, more initial notes could receive ratings, and the system could achieve at least some scalability.<br>\n<br>\nHowever, there are reasons to believe that using its current standards more members could actually make Birdwatch *less* scalable. <br>\n<br>\nThink back to the example of the tweet with the most initial notes ever. Lots of people wrote initial notes, but nowhere near enough people rated all those initial notes. <br>\n<br>\nIt's possible Birdwatch has better procedures in place now, but it seems like even more Birdwatch members could encounter a similar coordination problem. Too many people writing initial notes, and - after the initial probationary period - not enough people rating initial notes. That the vast majority of initial notes continue to have too few ratings to even be considered helpful does not inspire confidence.\n\n# Summary of Findings\n1. Birdwatch currently reviews an extraordinarily low volume of overall tweets <br>\n2. A vast majority of initial notes do not receive enough ratings to assess their helpfulness <br>\n3. Birdwatch is not likely to scale well in its current form\n\n# Suggested Action Items\n1. Pay people to perform Birdwatch's functions<br>\n<br>\nPaying people fair wages and professionalizing the approach to which tweets receive notes would likely yield a fantastic return on investment. This approach would help increase the volume of initial notes, ensure enough ratings for each initial note, and provide crucial coordination capabilities such as consistently prioritizing high-value tweets.^[I think the Birdwatch community has on balance elected to prioritize high-value tweets such as misleading tweets from Twitter's owner. I'm also certain Twitter has more data that could help a system like this nip misinformation in the bud *before* it's accrued millions of impressions] If Twitter wants to retain a bottom-up, participatory research style branch of content moderation they can do so. However, I recommend they consult with Trust and Safety professionals along with Birdwatch members on what form that participatory research should take. <br>\n<br>\n2. Re-situate Birdwatch as one of many content moderation tools<br>\n<br>\nBirdwatch doesn't need to massively scale to be useful. It could function well as a small part of a suite of content moderation tools. However, if Twitter relies on the current iteration of Birdwatch as its central content moderation tool I would expect massive content moderation failures. Not because Birdwatch community members are performing poorly, but because they're community members performing the tasks of many teams' worth of employees.\n\n# Conclusion\n\nThis analysis is only possible because Birdwatch has open data. Trust and Safety measures require some procedures be kept under lock and key, and the considered transparency baked into Birdwatch's approach since January 2021 is admirable.  The [now-defunct](https://gizmodo.com/twitter-layoffs-elon-musk-ai-ethics-1849743051){target=\"_blank\"} META team at Twitter also made [considered transparency a consistent practice](https://blog.twitter.com/en_us/topics/company/2021/introducing-responsible-machine-learning-initiative){target=\"_blank\"}. <br>\n<br>\nThe more teams follow these examples the better we'll be able to moderate content in helpful, just ways.<br>\n<br>\nThere are many stones still left to turn in this data. I hope I can inspire at least a couple of other people to take a closer look, and if you find anything interesting please [get in touch](https://mcmullarkey.github.io/){target=\"_blank\"}.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}