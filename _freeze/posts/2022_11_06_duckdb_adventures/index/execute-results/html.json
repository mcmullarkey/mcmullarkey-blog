{
  "hash": "f09c5dd31b7476041227911da756adf3",
  "result": {
    "markdown": "---\ntitle: Exploring My Twitter Archive with DuckDB and Python\ndescription: A Fun Way to Do In-Memory Data Exploration\nauthor: Michael Mullarkey\ndate: '2022-11-09'\nformat:\n  html:\n    code-fold: false\n    toc: true\ncategories:\n  - DuckDB\n  - Python\n  - Twitter\n---\n\n# Why Should I Care?\n\n[DuckDB](https://duckdb.org/why_duckdb.html#duckdbissimple){target=\"_blank\"} has been all the rage on Data Twitter^[Data Mastodon? Data 138-Different-Discord-Servers?] for ~983,000 Elon Musk attention spans. There are a lot of technical reasons to get excited,^[No external dependencies like SQLite with a lot more features, thorough testing with CI, open source] and I'll focus on one in particular - its ability to process data without copying it to a database.<br>\n<br>\nWorking with data that's just small enough to fit in local memory can feel like the worst of all worlds. You're using data wrangling tools designed for data that's smaller, but creating a database seems like overkill. Enter DuckDB, which allows you to [run SQL queries directly on Pandas data frames](https://duckdb.org/2021/05/14/sql-on-pandas.html){target=\"_blank\"}.^[As of me writing this I don't believe you can work directly on data frames or tibble in R, though you can interface with DuckDB using the dbplyr package https://duckdb.org/docs/api/r] <br>\n<br>\nNow you can take advantage of a blazing-fast^[Or maybe not, as we'll see soon!] style of SQL on local data. And since I love getting a bit meta, let's use this framework to analyze [my Twitter archive.](https://help.twitter.com/en/managing-your-account/how-to-download-your-twitter-archive){target=\"_blank\"}\n\n# Load Packages\n\nFirst we'll import the packages we need. If you want to skip the local timezone wrangling part you can eliminate the datetime, pytz, and tzlocal dependencies.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import packages, most for parsing the tweets.js file\n\nimport numpy as np\nimport pandas as pd\nimport duckdb\nimport json\nimport time\nimport datetime\nimport pytz\nfrom dateutil.tz import tzlocal\n```\n:::\n\n\n# Parsing My Tweets Data\n\nWe'll create a function to can load the tweets.js in as a pandas data frame. While the Javascript file was a little tricky at first I realized it was JSON with some extra text at the front. <br>\n<br>\nOpening the file, removing that text, and then normalizing the JSON let's us read in the tweets data as a Pandas Data Frame without too much fuss.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Adapting a parsing function from here but extended it to go into a DF \n# https://github.com/dangoldin/twitter-archive-analysis/blob/master/analyze.py\n\ndef load_tweets_from_js(js_file):\n    with open(js_file, \"r\") as f:\n        data = f.read()\n        data = data.replace(\"window.YTD.tweets.part0 = \", \"\")\n        tweets = json.loads(data)\n        for tweet in tweets:\n            ts = datetime.datetime.strptime(\n                tweet[\"tweet\"][\"created_at\"], \"%a %b %d %H:%M:%S +0000 %Y\"\n            )\n            ts = ts.replace(tzinfo=pytz.utc)\n            ts = ts.astimezone(tzlocal())\n            tweet[\"timestamp\"] = ts\n        print(\"Loaded %d tweets\" % len(tweets))\n        tweets_df = pd.json_normalize(tweets)\n        return tweets_df\n```\n:::\n\n\n# Some Initial Tidying with Pandas\n\nOne great aspect of using local data is we don't have to abandon other tools we know and love to exclusively use SQL. For example, we can use Pythonic methods to rename all our columns in a way that won't collide with the SQL interpreter^[Replacing all the \".\" in variable names with \"_\" so the SQL interpreter doesn't think we're referencing a variable within another table] in a single line of code.\n\n::: {.cell warnings='hide' execution_count=3}\n``` {.python .cell-code}\n# Use the function to create a Pandas DataFrame\n\ntweets = load_tweets_from_js(\"tweets.js\")\n\n# Replace periods with underscores so they don't confuse SQL later\n\ntweets.columns = tweets.columns.str.replace(\".\", \"_\",regex = False)\n\n# Convert multiple columns to numeric at once\n# Only doing with two columns here but could work with many more\n\nsecret_numeric = [\"tweet_favorite_count\",\"tweet_retweet_count\"]\n\ntweets[secret_numeric] = tweets[secret_numeric].apply(pd.to_numeric)\n\n# Confirm the conversion worked (Commented out because it did!)\n# tweets.dtypes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded 5244 tweets\n```\n:::\n:::\n\n\n# An Example DuckDB Query on a Pandas Data Frame\n\nNow we can use DuckDB as a querying engine directly on this pandas data frame! <br>\n<br>\nHere's a simple example to start, where we use DuckDB to (*gulp*) look at how many likes my tweets have gotten on average:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(duckdb.query(\"SELECT AVG(tweet_favorite_count) as avg_favorite_count FROM tweets\").to_df())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   avg_favorite_count\n0            5.127002\n```\n:::\n:::\n\n\n# Comparing the Speed of DuckDB and Pandas on My Twitter Archive\n\nOk, so how long does this take on my non-enormous Twitter archive? Let's create a timing function first.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef time_duckdb(full_query):\n  start = time.time()\n  duckb_query = duckdb.query(full_query).to_df()\n  end = time.time()\n  print(round(end - start, 4))\n\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntime_duckdb(\"SELECT AVG(tweet_favorite_count) as avg_favorite_count FROM tweets\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0507\n```\n:::\n:::\n\n\nPretty quick! How does this simple DuckDB query compare to an equivalent Pandas operation?\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nstart_pd_simple = time.time()\ntest_pandas_avg = tweets.agg(avg_favorite_count = (\"tweet_favorite_count\", \"mean\"))\nend_pd_simple = time.time()\nprint(round(end_pd_simple - start_pd_simple, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0039\n```\n:::\n:::\n\n\nOn simple queries on non-enormous data^[In this case ~10 MB] Pandas seems to still be much faster than DuckDB. That lead would diminish and [eventually disappear on larger data.](https://duckdb.org/2021/05/14/sql-on-pandas.html){target=\"_blank\"}<br>\n<br>\n\n# Sometime We Might Prefer to Write SQL\n\nAnd even if Pandas is faster on data of this size sometimes we might prefer to use DuckDB anyway. <br>\n<br>\nFor example, the syntax for doing a filtered \"group by\" using SQL is more intuitive to me than the equivalent Pandas syntax. Either works, and as we can see here Pandas is still faster on data at this size!\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntime_duckdb(\"SELECT AVG(tweet_favorite_count) as avg_favorite_count FROM tweets WHERE tweet_retweet_count > 10 GROUP BY tweet_truncated\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0497\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nstart = time.time()\nfiltered_pandas = tweets[tweets[\"tweet_retweet_count\"] >= 1]\ngrouped_avg_pandas = filtered_pandas.groupby('tweet_truncated', as_index=False).agg(avg_favorite_count = (\"tweet_favorite_count\", \"mean\"))\nend = time.time()\nprint(round(end - start, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0057\n```\n:::\n:::\n\n\nAnd since in this case the speed differential is negligible^[Both styles of queries are more than fast enough!] having another option for data wrangling can be handy. <br>\n<br>\nDuckDB also has the virtue of being far more scalable than Pandas. That scalability extends to much larger data, much more complex data wrangling, and both of those complexities combined.\n\n# Conclusion\n\nI had a good time exploring a more compact use-case for DuckDB. I was a bit surprised how much faster the Pandas queries ran vs. DuckDB at \"small-ish data\" size. I figured the two appraoaches would be closer in speed, though this may be a quirk of running Python through Quarto. <br>\n<br>\nI'm hoping this is just a warm-up act for my journey with DuckDB! I'm especially excited about DuckDB as a key catalyst for running the Modern Data Stack on a laptop, and if you're also intrigued I recommend checking out [this proof-of-concept](https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html){target=\"_blank\"} by [Jacob Matson](https://twitter.com/matsonj){target=\"_blank\"}. <br>\n<br>\nIf you have any questions or comments I'd love to hear from you! For at least a while [I'm on Twitter](https://twitter.com/mcmullarkey){target=\"_blank\"}, and other contact info is on [my website](https://mcmullarkey.github.io/){target=\"_blank\"}.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}